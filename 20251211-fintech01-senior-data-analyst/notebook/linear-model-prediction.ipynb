{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "8085f17d",
      "metadata": {},
      "source": [
        "# Data Transformation Pipeline for Logistic Regression Model\n",
        "\n",
        "**Source Notebook:** `yogi_takehome-test_fintech01_senior-data-analyst.ipynb`\n",
        "\n",
        "This notebook contains all data transformation steps extracted from the original analysis notebook, organized into distinct sections for easy understanding and reproduction.\n",
        "\n",
        "---\n",
        "\n",
        "## Table of Contents\n",
        "\n",
        "1. [Date Conversion & Temporal Feature Engineering](#1-date-conversion--temporal-feature-engineering)\n",
        "2. [Loan Term Grouping](#2-loan-term-grouping)\n",
        "3. [Loan Purpose Text Cleaning](#3-loan-purpose-text-cleaning)\n",
        "4. [Loan Purpose Standardization (Keyword Matching)](#4-loan-purpose-standardization-keyword-matching)\n",
        "5. [Loan Purpose Grouping (Top 5 + Others)](#5-loan-purpose-grouping-top-5--others)\n",
        "6. [Employment Type Text Cleaning](#6-employment-type-text-cleaning)\n",
        "7. [Model Feature Selection](#7-model-feature-selection)\n",
        "8. [Quarter Feature Extraction & Month Column Removal](#8-quarter-feature-extraction--month-column-removal)\n",
        "9. [One-Hot Encoding (Categorical Variables)](#9-one-hot-encoding-categorical-variables)\n",
        "10. [Merge Application & Credit Features DataFrames](#10-merge-application--credit-features-dataframes)\n",
        "11. [Filter Out Zero Credit Account Records](#11-filter-out-zero-credit-account-records)\n",
        "12. [Drop Single-Value Column](#12-drop-single-value-column-all_timesincemostrecentdefault)\n",
        "13. [Variance Inflation Factor (VIF) Analysis & Iterative Feature Removal](#13-variance-inflation-factor-vif-analysis--iterative-feature-removal)\n",
        "14. [Outlier Removal (Z-Score Method)](#14-outlier-removal-z-score-method)\n",
        "15. [Final Cleaned Dataset Summary](#15-final-cleaned-dataset-summary)\n",
        "\n",
        "---\n",
        "\n",
        "## Setup & Data Loading"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "2912136a",
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "pd.set_option('display.max_columns', 100)\n",
        "plt.style.use('seaborn-v0_8')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "b25aa74d",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(8847, 7)\n",
            "(8847, 14)\n"
          ]
        }
      ],
      "source": [
        "APPLICATION_PATH = '../data/application_samples.csv'\n",
        "CREDIT_PATH = '../data/credit_features.csv'\n",
        "\n",
        "application_df = pd.read_csv(APPLICATION_PATH)\n",
        "credit_df = pd.read_csv(CREDIT_PATH)\n",
        "\n",
        "print(application_df.shape)\n",
        "print(credit_df.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "846fda00",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Data transformation pipeline organized into 15 distinct sections below.\n",
        "# Each section handles a specific transformation step for the logistic regression model."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "53be3f00",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 1. Date Conversion & Temporal Feature Engineering\n",
        "\n",
        "**Purpose:** Convert string dates to datetime format and extract temporal features (Month, Quarter) for time-based analysis.\n",
        "\n",
        "**Columns Created:**\n",
        "- `ApplicationDate` â†’ converted to datetime\n",
        "- `Month` â†’ first day of the month (for grouping)\n",
        "- `Quarter` â†’ quarter string (e.g., \"2020Q1\")\n",
        "- `SuccessLabel` â†’ human-readable label (\"Success\" / \"Fail\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "28a240e1",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Shape after preprocessing: (8847, 10)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ApplicationDate</th>\n",
              "      <th>Month</th>\n",
              "      <th>Quarter</th>\n",
              "      <th>Success</th>\n",
              "      <th>SuccessLabel</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2020-07-03</td>\n",
              "      <td>2020-07-01</td>\n",
              "      <td>2020Q3</td>\n",
              "      <td>0</td>\n",
              "      <td>Fail</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2020-02-04</td>\n",
              "      <td>2020-02-01</td>\n",
              "      <td>2020Q1</td>\n",
              "      <td>0</td>\n",
              "      <td>Fail</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2020-08-02</td>\n",
              "      <td>2020-08-01</td>\n",
              "      <td>2020Q3</td>\n",
              "      <td>0</td>\n",
              "      <td>Fail</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2020-09-23</td>\n",
              "      <td>2020-09-01</td>\n",
              "      <td>2020Q3</td>\n",
              "      <td>0</td>\n",
              "      <td>Fail</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2020-01-01</td>\n",
              "      <td>2020-01-01</td>\n",
              "      <td>2020Q1</td>\n",
              "      <td>0</td>\n",
              "      <td>Fail</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  ApplicationDate      Month Quarter  Success SuccessLabel\n",
              "0      2020-07-03 2020-07-01  2020Q3        0         Fail\n",
              "1      2020-02-04 2020-02-01  2020Q1        0         Fail\n",
              "2      2020-08-02 2020-08-01  2020Q3        0         Fail\n",
              "3      2020-09-23 2020-09-01  2020Q3        0         Fail\n",
              "4      2020-01-01 2020-01-01  2020Q1        0         Fail"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def preprocess_applications(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"Clean and engineer base application fields.\"\"\"\n",
        "    df = df.copy()\n",
        "    \n",
        "    # Convert date string to datetime\n",
        "    df['ApplicationDate'] = pd.to_datetime(df['ApplicationDate'], dayfirst=True)\n",
        "    \n",
        "    # Ensure numeric types for Amount and Term\n",
        "    df['Amount'] = pd.to_numeric(df['Amount'], errors='coerce')\n",
        "    df['Term'] = pd.to_numeric(df['Term'], errors='coerce')\n",
        "    \n",
        "    # Extract Month (first day of month for grouping)\n",
        "    df['Month'] = df['ApplicationDate'].dt.to_period('M').dt.to_timestamp()\n",
        "    \n",
        "    # Extract Quarter as string (e.g., \"2020Q1\")\n",
        "    df['Quarter'] = df['ApplicationDate'].dt.to_period('Q').astype(str)\n",
        "    \n",
        "    # Create human-readable Success label\n",
        "    df['SuccessLabel'] = df['Success'].map({1: 'Success', 0: 'Fail'})\n",
        "    \n",
        "    return df\n",
        "\n",
        "application_df = preprocess_applications(application_df)\n",
        "print(f\"Shape after preprocessing: {application_df.shape}\")\n",
        "application_df[['ApplicationDate', 'Month', 'Quarter', 'Success', 'SuccessLabel']].head()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "969b9f5f",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 2. Loan Term Grouping\n",
        "\n",
        "**Purpose:** Group individual loan terms into broader categories to reduce granularity and improve statistical power for less common terms.\n",
        "\n",
        "**Mapping Logic:**\n",
        "- 12 months â†’ \"12\"\n",
        "- 18, 24 months â†’ \"18-24\"\n",
        "- 30, 36 months â†’ \"30-36\"\n",
        "- 42, 48 months â†’ \"42-48\"\n",
        "- 54, 60 months â†’ \"54-60\"\n",
        "\n",
        "**Column Created:** `Term_grouped`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "d096dae7",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Term grouping distribution:\n",
            "Term_grouped\n",
            "12        875\n",
            "18-24    1390\n",
            "30-36    1594\n",
            "42-48    2196\n",
            "54-60    2792\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "def group_terms(term):\n",
        "    \"\"\"Group loan terms into broader categories.\"\"\"\n",
        "    if term == 12:\n",
        "        return '12'\n",
        "    elif term in [18, 24]:\n",
        "        return '18-24'\n",
        "    elif term in [30, 36]:\n",
        "        return '30-36'\n",
        "    elif term in [42, 48]:\n",
        "        return '42-48'\n",
        "    elif term in [54, 60]:\n",
        "        return '54-60'\n",
        "    else:\n",
        "        return str(term)\n",
        "\n",
        "application_df['Term_grouped'] = application_df['Term'].apply(group_terms)\n",
        "\n",
        "print(\"Term grouping distribution:\")\n",
        "print(application_df['Term_grouped'].value_counts().sort_index())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "35e516cd",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 3. Loan Purpose Text Cleaning\n",
        "\n",
        "**Purpose:** Standardize raw loan purpose text by converting to lowercase and removing leading/trailing whitespace.\n",
        "\n",
        "**Transformation:**\n",
        "- Strip whitespace\n",
        "- Convert to lowercase\n",
        "\n",
        "**Column Created:** `LoanPurpose_clean`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "1e35e835",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Unique raw loan purposes: 2444\n",
            "Unique cleaned loan purposes: 2041\n"
          ]
        }
      ],
      "source": [
        "# Step 1: Lowercase and strip whitespace\n",
        "application_df[\"LoanPurpose_clean\"] = (\n",
        "    application_df[\"LoanPurpose\"]\n",
        "    .astype(str)\n",
        "    .str.strip()\n",
        "    .str.lower()\n",
        ")\n",
        "\n",
        "print(f\"Unique raw loan purposes: {application_df['LoanPurpose'].nunique()}\")\n",
        "print(f\"Unique cleaned loan purposes: {application_df['LoanPurpose_clean'].nunique()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9267a119",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 4. Loan Purpose Standardization (Keyword Matching)\n",
        "\n",
        "**Purpose:** Map similar loan purpose phrases to standardized categories using keyword matching to consolidate variations (e.g., \"debt consolidation\", \"consolidate debts\", \"debt consolidate\" â†’ \"debt consolidation\").\n",
        "\n",
        "**Keyword Mappings:**\n",
        "- Contains \"consolidat\" or \"debt\" â†’ \"debt consolidation\"\n",
        "- Contains \"home\" AND \"improv\" â†’ \"home improvement\"\n",
        "- Otherwise â†’ keep original cleaned value\n",
        "\n",
        "**Column Created:** `LoanPurpose_standardized`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "43a52744",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Unique standardized loan purposes: 1674\n",
            "\n",
            "Top 10 standardized loan purposes:\n",
            "LoanPurpose_standardized\n",
            "debt consolidation    2072\n",
            "car                   1372\n",
            "home improvement       890\n",
            "other                  371\n",
            "new car                149\n",
            "furniture              117\n",
            "car purchase            85\n",
            "car repairs             77\n",
            "short term loan         70\n",
            "car loan                64\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "def standardize_loan_purpose(purpose):\n",
        "    \"\"\"Map loan purpose variations to standardized categories using keyword matching.\"\"\"\n",
        "    purpose = purpose.lower()\n",
        "    \n",
        "    # Debt consolidation variants\n",
        "    if any(word in purpose for word in ['consolidat', 'debt']):\n",
        "        return 'debt consolidation'\n",
        "    \n",
        "    # Home improvement variants\n",
        "    if 'home' in purpose and 'improv' in purpose:\n",
        "        return 'home improvement'\n",
        "    \n",
        "    # Keep as-is for unmatched\n",
        "    return purpose\n",
        "\n",
        "application_df[\"LoanPurpose_standardized\"] = (\n",
        "    application_df[\"LoanPurpose_clean\"]\n",
        "    .apply(standardize_loan_purpose)\n",
        ")\n",
        "\n",
        "print(f\"Unique standardized loan purposes: {application_df['LoanPurpose_standardized'].nunique()}\")\n",
        "print(\"\\nTop 10 standardized loan purposes:\")\n",
        "print(application_df['LoanPurpose_standardized'].value_counts().head(10))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2f6bb759",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 5. Loan Purpose Grouping (Top 5 + Others)\n",
        "\n",
        "**Purpose:** Reduce cardinality by keeping only the top 5 most frequent loan purposes and grouping all others into \"others\" category.\n",
        "\n",
        "**Logic:**\n",
        "- Keep top 5 standardized loan purposes by frequency\n",
        "- All other purposes â†’ \"others\"\n",
        "\n",
        "**Column Created:** `LoanPurpose_grouped`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "cd289d78",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loan Purpose Grouped Distribution:\n",
            "LoanPurpose_grouped\n",
            "others                4247\n",
            "debt consolidation    2072\n",
            "car                   1372\n",
            "home improvement       890\n",
            "new car                149\n",
            "furniture              117\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "# Get top 5 standardized loan purposes (excluding \"other\" if present)\n",
        "std_top5 = (\n",
        "    application_df[application_df[\"LoanPurpose_standardized\"] != \"other\"][\"LoanPurpose_standardized\"]\n",
        "        .value_counts()\n",
        "        .head(5)\n",
        ")\n",
        "std_top5_names = std_top5.index.tolist()\n",
        "\n",
        "# Create grouped column: top 5 + others\n",
        "application_df[\"LoanPurpose_grouped\"] = application_df[\"LoanPurpose_standardized\"].apply(\n",
        "    lambda x: x if x in std_top5_names else \"others\"\n",
        ")\n",
        "\n",
        "print(\"Loan Purpose Grouped Distribution:\")\n",
        "print(application_df['LoanPurpose_grouped'].value_counts())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "79540a7a",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 7. Model Feature Selection\n",
        "\n",
        "**Purpose:** Select relevant columns from application dataframe for the logistic regression model.\n",
        "\n",
        "**Columns Selected:**\n",
        "- `UID` - unique identifier for merging\n",
        "- `Month` - temporal feature (will be converted to Quarter)\n",
        "- `Amount` - loan amount (continuous)\n",
        "- `Term_grouped` - grouped loan term (categorical)\n",
        "- `EmploymentType` - employment type (categorical)\n",
        "- `LoanPurpose_grouped` - grouped loan purpose (categorical)\n",
        "- `Success` - target variable (binary)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "efc2427d",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model DataFrame shape: (8847, 7)\n",
            "\n",
            "Columns selected: ['UID', 'Month', 'Amount', 'Term_grouped', 'EmploymentType', 'LoanPurpose_grouped', 'Success']\n"
          ]
        }
      ],
      "source": [
        "# Select columns for modeling\n",
        "application_model_df = application_df[['UID', 'Month', 'Amount', 'Term_grouped', 'EmploymentType', 'LoanPurpose_grouped', 'Success']]\n",
        "\n",
        "print(f\"Model DataFrame shape: {application_model_df.shape}\")\n",
        "print(f\"\\nColumns selected: {list(application_model_df.columns)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "777a78da",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 8. Quarter Feature Extraction & Month Column Removal\n",
        "\n",
        "**Purpose:** Extract Quarter from Month column for model features, then drop the Month column (too granular for the model).\n",
        "\n",
        "**Transformation:**\n",
        "- Extract quarter number (1-4) from Month datetime\n",
        "- Drop Month column\n",
        "\n",
        "**Column Created:** `Quarter` (integer 1-4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "4dc4ffd6",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Shape after Quarter extraction: (8847, 7)\n",
            "Columns: ['UID', 'Amount', 'Term_grouped', 'EmploymentType', 'LoanPurpose_grouped', 'Success', 'Quarter']\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/hf/0mp7k_vd30z8kr8nkwwh037m0000gn/T/ipykernel_63058/3936759089.py:2: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  application_model_df[\"Quarter\"] = application_model_df[\"Month\"].dt.quarter\n"
          ]
        }
      ],
      "source": [
        "# Create Quarter column from Month (integer 1-4)\n",
        "application_model_df[\"Quarter\"] = application_model_df[\"Month\"].dt.quarter\n",
        "\n",
        "# Drop the Month column (too granular for model)\n",
        "application_model_df = application_model_df.drop(columns=[\"Month\"])\n",
        "\n",
        "print(f\"Shape after Quarter extraction: {application_model_df.shape}\")\n",
        "print(f\"Columns: {list(application_model_df.columns)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "53e02813",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 9. One-Hot Encoding (Categorical Variables)\n",
        "\n",
        "**Purpose:** Convert categorical variables into binary dummy variables for logistic regression. Uses `drop_first=True` to avoid the dummy variable trap (multicollinearity).\n",
        "\n",
        "**Columns Encoded:**\n",
        "- `Term_grouped` â†’ Term_grouped_18-24, Term_grouped_30-36, etc.\n",
        "- `EmploymentType` â†’ EmploymentType_Employed - part time, EmploymentType_Retired, etc.\n",
        "- `LoanPurpose_grouped` â†’ LoanPurpose_grouped_debt consolidation, etc.\n",
        "- `Quarter` â†’ Quarter_2, Quarter_3, Quarter_4\n",
        "\n",
        "**Note:** First category of each variable is dropped as reference (baseline)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "5fec6c14",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "New columns after one-hot encoding:\n",
            "['Term_grouped_18-24', 'Term_grouped_30-36', 'Term_grouped_42-48', 'Term_grouped_54-60', 'EmploymentType_Employed - part time', 'EmploymentType_Retired', 'EmploymentType_Self employed', 'LoanPurpose_grouped_debt consolidation', 'LoanPurpose_grouped_furniture', 'LoanPurpose_grouped_home improvement', 'LoanPurpose_grouped_new car', 'LoanPurpose_grouped_others', 'Quarter_2', 'Quarter_3', 'Quarter_4']\n",
            "\n",
            "DataFrame shape after encoding: (8847, 18)\n"
          ]
        }
      ],
      "source": [
        "# One-hot encode categorical variables\n",
        "application_model_df = pd.get_dummies(\n",
        "    application_model_df,\n",
        "    columns=['Term_grouped', 'EmploymentType', 'LoanPurpose_grouped', 'Quarter'],\n",
        "    drop_first=True,  # Avoid dummy variable trap (multicollinearity)\n",
        "    dtype=int  # Use 0/1 integers instead of True/False\n",
        ")\n",
        "\n",
        "# View the new columns created\n",
        "print(\"New columns after one-hot encoding:\")\n",
        "encoded_cols = [col for col in application_model_df.columns \n",
        "                if 'Term_' in col or 'EmploymentType_' in col \n",
        "                or 'LoanPurpose_grouped_' in col or 'Quarter_' in col]\n",
        "print(encoded_cols)\n",
        "\n",
        "print(f\"\\nDataFrame shape after encoding: {application_model_df.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e7283351",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 10. Merge Application & Credit Features DataFrames\n",
        "\n",
        "**Purpose:** Combine application features with credit bureau features on the unique identifier (UID) to create a comprehensive feature set.\n",
        "\n",
        "**Join Type:** Inner join (only keep records that exist in both datasets)\n",
        "\n",
        "**Columns Added from credit_df:**\n",
        "- ALL_AgeOfOldestAccount, ALL_AgeOfYoungestAccount\n",
        "- ALL_Count, ALL_CountActive, ALL_CountSettled\n",
        "- ALL_CountClosedLast12Months, ALL_CountDefaultAccounts, ALL_CountOpenedLast12Months\n",
        "- ALL_MeanAccountAge\n",
        "- ALL_SumCurrentOutstandingBal, ALL_SumCurrentOutstandingBalExcMtg\n",
        "- ALL_TimeSinceMostRecentDefault\n",
        "- ALL_WorstPaymentStatusActiveAccounts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "4e03195c",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Shape after merge: (8847, 31)\n",
            "Columns: ['UID', 'Amount', 'Success', 'Term_grouped_18-24', 'Term_grouped_30-36', 'Term_grouped_42-48', 'Term_grouped_54-60', 'EmploymentType_Employed - part time', 'EmploymentType_Retired', 'EmploymentType_Self employed', 'LoanPurpose_grouped_debt consolidation', 'LoanPurpose_grouped_furniture', 'LoanPurpose_grouped_home improvement', 'LoanPurpose_grouped_new car', 'LoanPurpose_grouped_others', 'Quarter_2', 'Quarter_3', 'Quarter_4', 'ALL_AgeOfOldestAccount', 'ALL_AgeOfYoungestAccount', 'ALL_Count', 'ALL_CountActive', 'ALL_CountClosedLast12Months', 'ALL_CountDefaultAccounts', 'ALL_CountOpenedLast12Months', 'ALL_CountSettled', 'ALL_MeanAccountAge', 'ALL_SumCurrentOutstandingBal', 'ALL_SumCurrentOutstandingBalExcMtg', 'ALL_TimeSinceMostRecentDefault', 'ALL_WorstPaymentStatusActiveAccounts']\n"
          ]
        }
      ],
      "source": [
        "# Merge application features with credit bureau features\n",
        "combined_model_features_df = pd.merge(\n",
        "    application_model_df, \n",
        "    credit_df, \n",
        "    on='UID', \n",
        "    how='inner'\n",
        ")\n",
        "\n",
        "print(f\"Shape after merge: {combined_model_features_df.shape}\")\n",
        "print(f\"Columns: {list(combined_model_features_df.columns)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fc14a651",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 11. Filter Out Zero Credit Account Records\n",
        "\n",
        "**Purpose:** Remove applicants with no credit history (ALL_Count = 0) as they:\n",
        "1. Have 100% rejection rate (perfect separation issue)\n",
        "2. Violate logistic regression assumptions\n",
        "3. Should be routed to alternative review process\n",
        "\n",
        "**Filter Condition:** Keep rows where `ALL_Count != 0`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "96b84d04",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Records with ALL_Count = 0: 108\n",
            "Shape after filtering: (8739, 31)\n"
          ]
        }
      ],
      "source": [
        "# Count records with zero credit accounts\n",
        "zero_credit_count = (combined_model_features_df['ALL_Count'] == 0).sum()\n",
        "print(f\"Records with ALL_Count = 0: {zero_credit_count}\")\n",
        "\n",
        "# Filter out zero credit account records\n",
        "combined_model_features_df = combined_model_features_df[combined_model_features_df['ALL_Count'] != 0]\n",
        "\n",
        "print(f\"Shape after filtering: {combined_model_features_df.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "42cd2ca0",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 12. Drop Single-Value Column (ALL_TimeSinceMostRecentDefault)\n",
        "\n",
        "**Purpose:** Remove columns that have only one unique value (constant), as they provide no predictive information.\n",
        "\n",
        "**Column Dropped:** `ALL_TimeSinceMostRecentDefault`\n",
        "\n",
        "**Reason:** After filtering out zero credit accounts, this column may have only one value (or very limited variance)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "71cfe6f0",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Shape after dropping single-value column: (8739, 30)\n"
          ]
        }
      ],
      "source": [
        "# Drop column with single value (no predictive power)\n",
        "combined_model_features_df = combined_model_features_df.drop('ALL_TimeSinceMostRecentDefault', axis=1)\n",
        "\n",
        "print(f\"Shape after dropping single-value column: {combined_model_features_df.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "53e8d6b4",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 13. Variance Inflation Factor (VIF) Analysis & Iterative Feature Removal\n",
        "\n",
        "**Purpose:** Detect and remove multicollinear features using Variance Inflation Factor (VIF). High multicollinearity inflates standard errors and makes coefficient interpretation unreliable.\n",
        "\n",
        "**VIF Interpretation:**\n",
        "- VIF > 10: High multicollinearity - Remove\n",
        "- VIF > 5: Moderate - Consider removing\n",
        "- VIF â‰¤ 5: OK - Keep\n",
        "\n",
        "**Process:**\n",
        "1. Calculate VIF for all features\n",
        "2. Remove feature with highest VIF if VIF > threshold\n",
        "3. Repeat until all VIF â‰¤ threshold\n",
        "\n",
        "**Threshold Used:** VIF > 7.5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "51062d07",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Iterative VIF Analysis - Removing One Feature at a Time\n",
            "================================================================================\n",
            "Threshold: VIF > 7.5\n",
            "Starting features: 28\n",
            "================================================================================\n",
            "\n",
            "ðŸ”„ Iteration 1: Highest VIF = 560,152.45 (ALL_Count)\n",
            "   âŒ Removed: ALL_Count\n",
            "\n",
            "ðŸ”„ Iteration 2: Highest VIF = 18.34 (ALL_MeanAccountAge)\n",
            "   âŒ Removed: ALL_MeanAccountAge\n",
            "\n",
            "ðŸ”„ Iteration 3: Highest VIF = 9.30 (ALL_CountActive)\n",
            "   âŒ Removed: ALL_CountActive\n",
            "\n",
            "âœ… Iteration 4: All VIF values now â‰¤ 7.5\n",
            "\n",
            "================================================================================\n",
            "ðŸ“Š SUMMARY\n",
            "================================================================================\n",
            "Features removed (3):\n",
            "   1. ALL_Count\n",
            "   2. ALL_MeanAccountAge\n",
            "   3. ALL_CountActive\n",
            "\n",
            "Remaining features: 25\n"
          ]
        }
      ],
      "source": [
        "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
        "\n",
        "def calculate_vif(X, feature_cols):\n",
        "    \"\"\"Calculate VIF for all features.\"\"\"\n",
        "    vif_data = pd.DataFrame()\n",
        "    vif_data[\"Feature\"] = feature_cols\n",
        "    vif_data[\"VIF\"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n",
        "    return vif_data.sort_values(\"VIF\", ascending=False).reset_index(drop=True)\n",
        "\n",
        "def vif_status(vif):\n",
        "    \"\"\"Return status label based on VIF value.\"\"\"\n",
        "    if vif > 10:\n",
        "        return \"âŒ High - Remove\"\n",
        "    elif vif > 5:\n",
        "        return \"âš ï¸ Moderate - Consider\"\n",
        "    else:\n",
        "        return \"âœ… OK\"\n",
        "\n",
        "# Get feature columns (exclude UID and Success)\n",
        "feature_cols = [col for col in combined_model_features_df.columns if col not in ['UID', 'Success']]\n",
        "X = combined_model_features_df[feature_cols].copy()\n",
        "\n",
        "# Iterative VIF removal\n",
        "VIF_THRESHOLD = 7.5\n",
        "removed_features = []\n",
        "iteration = 0\n",
        "\n",
        "print(\"Iterative VIF Analysis - Removing One Feature at a Time\")\n",
        "print(\"=\" * 80)\n",
        "print(f\"Threshold: VIF > {VIF_THRESHOLD}\")\n",
        "print(f\"Starting features: {len(feature_cols)}\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "while True:\n",
        "    iteration += 1\n",
        "    current_features = [col for col in feature_cols if col not in removed_features]\n",
        "    X_current = combined_model_features_df[current_features]\n",
        "    \n",
        "    vif_data = calculate_vif(X_current, current_features)\n",
        "    max_vif = vif_data[\"VIF\"].max()\n",
        "    max_vif_feature = vif_data.loc[vif_data[\"VIF\"].idxmax(), \"Feature\"]\n",
        "    \n",
        "    if max_vif <= VIF_THRESHOLD:\n",
        "        print(f\"\\nâœ… Iteration {iteration}: All VIF values now â‰¤ {VIF_THRESHOLD}\")\n",
        "        break\n",
        "    \n",
        "    print(f\"\\nðŸ”„ Iteration {iteration}: Highest VIF = {max_vif:,.2f} ({max_vif_feature})\")\n",
        "    removed_features.append(max_vif_feature)\n",
        "    print(f\"   âŒ Removed: {max_vif_feature}\")\n",
        "\n",
        "# Final summary\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"ðŸ“Š SUMMARY\")\n",
        "print(\"=\" * 80)\n",
        "print(f\"Features removed ({len(removed_features)}):\")\n",
        "for i, feat in enumerate(removed_features, 1):\n",
        "    print(f\"   {i}. {feat}\")\n",
        "\n",
        "# Store cleaned feature list\n",
        "feature_cols_cleaned = [col for col in feature_cols if col not in removed_features]\n",
        "print(f\"\\nRemaining features: {len(feature_cols_cleaned)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c379316b",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 14. Outlier Removal (Z-Score Method)\n",
        "\n",
        "**Purpose:** Remove extreme outliers from monetary columns that could unduly influence the logistic regression model.\n",
        "\n",
        "**Method:** Z-Score based removal\n",
        "- Calculate mean and standard deviation\n",
        "- Remove rows where |value - mean| > 3 Ã— standard deviation\n",
        "- This corresponds to approximately 0.3% of data in a normal distribution\n",
        "\n",
        "**Column Targeted:** `ALL_SumCurrentOutstandingBal`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "2cfca08e",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Outlier Removal Summary (ALL_SumCurrentOutstandingBal):\n",
            "  Rows before: 8739\n",
            "  Rows after: 8621\n",
            "  Rows removed: 118 (1.35%)\n"
          ]
        }
      ],
      "source": [
        "# Remove outliers where Z-score > 3 for ALL_SumCurrentOutstandingBal\n",
        "col = 'ALL_SumCurrentOutstandingBal'\n",
        "mean = combined_model_features_df[col].mean()\n",
        "std = combined_model_features_df[col].std()\n",
        "\n",
        "# Count before\n",
        "rows_before = len(combined_model_features_df)\n",
        "\n",
        "# Keep rows where Z-score <= 3\n",
        "combined_model_features_df = combined_model_features_df[\n",
        "    ((combined_model_features_df[col] - mean).abs() <= 3 * std)\n",
        "]\n",
        "\n",
        "# Count after\n",
        "rows_after = len(combined_model_features_df)\n",
        "rows_removed = rows_before - rows_after\n",
        "\n",
        "print(f\"Outlier Removal Summary ({col}):\")\n",
        "print(f\"  Rows before: {rows_before}\")\n",
        "print(f\"  Rows after: {rows_after}\")\n",
        "print(f\"  Rows removed: {rows_removed} ({rows_removed/rows_before*100:.2f}%)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f1151a1f",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 15. Final Cleaned Dataset Summary\n",
        "\n",
        "**Purpose:** Display the final shape and columns of the cleaned dataset ready for logistic regression modeling."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "3d6233df",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "FINAL CLEANED DATASET SUMMARY\n",
            "================================================================================\n",
            "Total rows: 8621\n",
            "Total columns: 30\n",
            "\n",
            "Feature columns (for modeling): 25\n",
            "Target column: Success\n",
            "\n",
            "Features ready for Logistic Regression:\n",
            "   1. Amount\n",
            "   2. Term_grouped_18-24\n",
            "   3. Term_grouped_30-36\n",
            "   4. Term_grouped_42-48\n",
            "   5. Term_grouped_54-60\n",
            "   6. EmploymentType_Employed - part time\n",
            "   7. EmploymentType_Retired\n",
            "   8. EmploymentType_Self employed\n",
            "   9. LoanPurpose_grouped_debt consolidation\n",
            "  10. LoanPurpose_grouped_furniture\n",
            "  11. LoanPurpose_grouped_home improvement\n",
            "  12. LoanPurpose_grouped_new car\n",
            "  13. LoanPurpose_grouped_others\n",
            "  14. Quarter_2\n",
            "  15. Quarter_3\n",
            "  16. Quarter_4\n",
            "  17. ALL_AgeOfOldestAccount\n",
            "  18. ALL_AgeOfYoungestAccount\n",
            "  19. ALL_CountClosedLast12Months\n",
            "  20. ALL_CountDefaultAccounts\n",
            "  21. ALL_CountOpenedLast12Months\n",
            "  22. ALL_CountSettled\n",
            "  23. ALL_SumCurrentOutstandingBal\n",
            "  24. ALL_SumCurrentOutstandingBalExcMtg\n",
            "  25. ALL_WorstPaymentStatusActiveAccounts\n",
            "\n",
            "âœ… X shape: (8621, 25)\n",
            "âœ… y shape: (8621,)\n"
          ]
        }
      ],
      "source": [
        "# Final dataset summary\n",
        "print(\"=\" * 80)\n",
        "print(\"FINAL CLEANED DATASET SUMMARY\")\n",
        "print(\"=\" * 80)\n",
        "print(f\"Total rows: {len(combined_model_features_df)}\")\n",
        "print(f\"Total columns: {len(combined_model_features_df.columns)}\")\n",
        "print(f\"\\nFeature columns (for modeling): {len(feature_cols_cleaned)}\")\n",
        "print(f\"Target column: Success\")\n",
        "print(f\"\\nFeatures ready for Logistic Regression:\")\n",
        "for i, col in enumerate(feature_cols_cleaned, 1):\n",
        "    print(f\"  {i:2d}. {col}\")\n",
        "\n",
        "# Prepare final X and y for modeling\n",
        "X_final = combined_model_features_df[feature_cols_cleaned]\n",
        "y_final = combined_model_features_df['Success']\n",
        "\n",
        "print(f\"\\nâœ… X shape: {X_final.shape}\")\n",
        "print(f\"âœ… y shape: {y_final.shape}\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "data-science",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
